.output chapter1.wd
.bookmark basics

+ 0MZ基础

++ 给这个世界打上补丁


要怎么介绍0MQ呢？有些人会形象的赞美它。//它是类固醇的通道，像自动送信的邮箱，速度还一级快！//有些人会谈谈那些顿悟的瞬间，和“重击——会心一击——致命一击！”一样，灵光一现，一切都如此清晰明了。//复杂的事变简单了，思路瞬间打开了！//还有些人会类比。//小巧简洁，但还是熟悉的味道。//就我个人而言，我更愿意分享一些关于当初我们为何要写0MQ的故事，因为那时，我们的处境就和读者你一模一样。

编程就是一门伪装成艺术的科学，因为大部分人不明白什么是软件的本质。就算被教育过，也真是少得可怜。软件的本质，不是算法，不是数据结构，不是编程语言也不是是抽象。这些仅仅是工具，我们制造、使用、然后丢弃。软件真正的本质在于人，换而言之，在于面对复杂问题时，我们是否愿意通过协作将其分而治之。这才是编程科学：造一些易懂//易用//的小模块，而大家可以协作解决难题。

我们生活在一个互联的世界，现代软件也需要能对应这个世界。我的意思是，为了解决未来更大的问题，我们的模块必须提供互联和高度并行的能力。“沉默而健壮”代码是不够的，代码必须健谈、相互关联，代码之间要能交流。在未来，代码要能和人脑一样，数以万亿的神经元高速的传递信息，高度并行，无须中央控制，没有节点故障，但仍能有效的解决问题。这一切并不意外，因为从某种意义上来讲，现在网络上的每个节点就像是一个人脑。

如果你曾用过线程、协议或者网络，你也许会说这是天方夜谭。解决实际问题中，用套接字连几个程序就足够麻烦了。数以万亿？别开玩笑了。要知道，也只有土豪公司才能担负得起用大量计算机互联来提供软件和服务。

所以，现实情况是我们根本没法驾驭这样的网络。80年代曾经有过一次软件危机，像Fred Brooks这样的软件开发先驱们相信[http://en.wikipedia.org/wiki/No_Silver_Bullet 没有银弹]能“让软件开发的生产率、可靠性或者简单性中某一项瞬间提高一个档次”。

Brooks没有预见到，自由和开源软件运动解决这次危机，让我们能更有效的分享知识。如今，我们又面临另一次软件危机，但是我们都没有过多的谈及它。只有最大、最富有的公司才能创造出互联的应用。当然了，谁都知道有云，但是那都不是全人类的云。我们的数据和知识从个人电脑中消失，转移到了我们无法接触的云上了，那我们还有什么资本？想想现在谁掌控着我们的社交网络？这简直就是大型机对个人电脑的逆袭。

我们暂且不管其中的的政治哲学，那都可以[http://swsi.info 另外出书]了。问题是互联网提供了让代码互联的潜力，现实是我们大部分人根本无从用起。而那些有趣的大规模问题（健康、教育、金融、交通等）却因为我们无法使用那些代码而迟而未决。假如我们根本没有办法连接到那些大脑，协同工作又从何谈起呢。

曾经有过很多对代码进行互联的尝试。有着成百上千的IETF(互联网工程任务组)草案，每一个都尝试解决某部分难题。对于应用开发者，HTTP可能是一个还算简单的方案，但我们也可以说它让情况更加恶化，因为该协议鼓励着开发者和架构师构建重量级的服务器、轻量却笨拙的客户端。

因此，时至今日，人们还在使用最原始的UDP、TCP、私有协议、HTTP、WebSockets来连接应用程序，这些工作缓慢而痛苦、扩展性差，而且基本上都是试图中心化。而分布式P2P协议却大多使用在娱乐领域而非工作之中。你见过谁用Skype或者Bittorrent来交换数据么？

残酷的现实再次将我们拉回到编程科学上来。为了修正这个bug，我们需要做两件事。其一，解决能在任何地点、让任意代码互联的问题。其二，将这一方法打包成易懂//易用//的、最最简单的程序模块提供给人们用。

这些听起来简单到荒谬！不过，也许真是这样。这些大概就是我想说的了。

++ 预先假定

我们假定你使用的0MQ版本大于3.2，用的是Linux或其他类似系统。你多少得懂一点C语言，因为示例程序默认情况下都是C代码。另外，当我们提及PUSH或SUBSCRIBE等常量时，你得知道程序真正用的其实是{{ZMQ_PUSH}}或{{ZMQ_SUBSCRIBE}}。

++ 获取示例

示例程序都在[https://github.com/imatix/zguide Github项目]中。最简单的获取方法：

[[code]]
git clone --depth=1 git://github.com/imatix/zguide.git
[[/code]]

接着，打开examples子目录。你可以按语言找到对应的示例。如果缺了你使用的编程语言，我们欢迎你[http://zguide.zeromq.org/main:translate 提交一份“翻译”]，正所谓众人拾柴火焰高。所有的示例代码都遵循MIT/X11许可。

## 请求、应答

那么，让我们从一些简单的代码开始。先是一个“Hello World”。我们将写一个client/server。client发送“Hello”给server，然后server返回“World”。这里是C代码实现的server，它在5555端口打开了一个0MQ套接字，从该端口接收请求，收到请求后应答“World”：
[[code type="example" title="Hello World server" name="hwserver" language="C"]]
[[/code]]

[[code type="textdiagram" title="Request-Reply"]]
  #------------#
  |   Client   |
  +------------+
  |    REQ     |
  '---+--------'
      |    ^
      |    |
Hello |    | World
      |    |
      v    |
  .--------+---.
  |    REP     |
  +------------+
  |   Server   |
  #------------#
[[/code]]

REQ-REP套接字对是完全同步的。client使用{{zmq_send[3]}}发送详细，然后不停等待{{zmq_recv[3]}}接受消息（或者只按需等待一次）。乱序（如连续发送两次消息）调用{{send}}或者{{recv}}都将导致返回错误-1。同理，服务也必须按照顺序，先{{zmq_recv[3]}}然后{{zmq_send[3]}}，循环等待。

0MQ使用C作为参考手册的语言，同时本指南中示例也都用的C。如果你是在线浏览，那么例子下面的链接提供了其他语言的“翻译”。让我们看一下C++版本的server：

[[code type="example" title="Hello World server" name="hwserver" language="C++"]]
[[/code]]

你会发现，C和C++版本的0MQ API非常的相似。而对于PHP或Java等语言，我们甚至可以隐藏起更多的细节，代码更为简洁：

[[code type="example" title="Hello World server" name="hwserver" language="PHP"]]
[[/code]]

[[code type="example" title="Hello World server" name="hwserver" language="Java"]]
[[/code]]

以下是client的代码：

[[code type="example" title="Hello World client" name="hwclient"]]
[[/code]]

看起来还不够实用？但是我们知道，0MQ套接字可是有超能力。你可以一次性抛出成千上百个client给这个server，而它仍然工作的欢快无比。好玩的是，你也可以先启动client，//然后//才启动server，看看它是否还工作。想想看，这意味着什么？

让我们简单的解释一下这两段程序。他们打开了一份0MQ context（上下文）以及一个socket（套接字）。先别急着弄懂这些术语，你会明白的。server绑定了一个5555号端口的REP（应答）套接字，然后在循环等待请求，请求到来后回复一个应答。而client就是给server发送请求并读取应答。

如果你杀死了server进程（Ctrl-C），然后重新启动它，client并不能正确的获得应答。想恢复挂掉的进程可没那么容易。我们将在[#可靠请求-应答]解决如何可靠的请求-应答这一难题。

这东西背后有不少机制，但是对我们最重要的是，这些代码短小精悍，高负荷下也不会轻易出错。这就是请求-应答模式，0MQ最简单的使用方法。它对应着RPC协议以及经典的client/server模型。

++ 关于字符串的一点注意事项

0MQ不知道你发送的是什么内容，它仅知道你发送了多长的内容。也就是说，你得自己对内容进行格式化，让应用程序能读懂你的内容。给对象或其他高级数据类型进行格式化，就需要一些协议上的支持了。但是就算你只发送字符串，你也得小心点。

在C和一些其他语言中，字符串都是以一个空字节结尾。我们可以发送附带一个空字节的字符串，比如"HELLO"：

[[code language="C"]]
zmq_send (requester, "Hello", 6, 0);
[[/code]]

但是，如果你用另一种语言，也许就没有结尾的那个空字节了。比如，在Python中，发送同样一个字符串，我们可以这样：

[[code language="Python"]]
socket.send ("Hello")
[[/code]]

然后，真正在管道都走的是长度（一字节）和几个独立的字符[figure]。

[[code type="textdiagram" title="A 0MQ string"]]
#-----#  #-----+-----+-----+-----+-----#
|  5  |  |  H  |  e  |  l  |  l  |  o  |
#-----#  #-----+-----+-----+-----+-----#
[[/code]]

而且，如果你用C程序来读它，你可能会得到一个字符串，或许碰巧是对的（5个字节之后跟着一个无辜的空字节），但是不是每次都这么幸运。如果你的client和server没有就字符串格式达成一致，结果可能会非常奇怪。

如果你在C里从0MQ获取字符串数据，你可不能相信它已经安全的结束了。每次你读取一个字符串，你都得分配大一字节的空间，将字符串复制进去，再在最后填一个空字节。

所以，定个规则，**0MQ的字符串都是定长的，在管道中//没有//多余的末尾空字节**。最简单的情况下，0MQ的字符串整齐的对应着一个0MQ帧，就像上图所示——一个长度，再加上一些字节。

在C中，我们要正确的接收一个字符串，再传给程序：

[[code language="C"]]
//  Receive 0MQ string from socket and convert into C string
//  Chops string at 255 chars, if it's longer
static char *
s_recv (void *socket) {
    char buffer [256];
    int size = zmq_recv (socket, buffer, 255, 0);
    if (size == -1)
        return NULL;
    if (size > 255)
        size = 255;
    buffer [size] = 0;
    return strdup (buffer);
}
[[/code]]

这是个方便的函数，秉承着重用原则，我们写了一个相似的{{s_send}}函数来正确的发送字符串，并打包进头文件以方便调用。

这就是{{zhelpers.h}}，我们可以更便捷的在C里写出更短的0MQ程序。源文件有点长，并且只对C开发者有用，[https://github.com/imatix/zguide/blob/master/examples/C/zhelpers.h 有空可以一读]。

++ 打印版本

0MQ已经有好几个版本了，而且更迭速度很快，如果你有什么问题，也许下一版就解决了。所以，知道当前使用的0MQ版本是很有用的。

这有个简单的程序：

[[code type="example" title="0MQ version reporting" name="version"]]
[[/code]]

++ 消息流

第二个经典的模式是单向发布，也就是一个server给一打client推送消息。比如，示例中发布天气预报，包含的数据有邮编、温度和湿度。我们随机生成一些数据，因为天气检测站点都这么干。

以下server，使用了5556号端口：

[[code type="example" title="Weather update server" name="wuserver"]]
[[/code]]

这里的数据流无始无终，和广播一样没有结束[figure]。

以下是client，监听来自指定邮编的数据。默认邮编为纽约，一个开始冒险旅途的好地方：

[[code type="example" title="Weather update client" name="wuclient"]]
[[/code]]

[[code type="textdiagram" title="Publish-Subscribe"]]
               #-------------#
               |  Publisher  |
               +-------------+
               |     PUB     |
               '-------------'
                    bind
                      |
                      |
                   updates
                      |
      .---------------+---------------.
      |               |               |
   updates         updates         updates
      |               |               |
      |               |               |
      v               v               v
   connect         connect         connect
.------------.  .------------.  .------------.
|    SUB     |  |    SUB     |  |    SUB     |
+------------+  +------------+  +------------+
| Subscriber |  | Subscriber |  | Subscriber |
#------------#  #------------#  #------------#
[[/code]]

需要注意的一点，和代码中一样，如果你要使用SUB套接字，你**必须**用{{zmq_setsockopt[3]}}给这个套接字增加一个订阅。如果你不设置任何订阅，你就不能获得任何消息。这是新人经常犯的错。订阅者可以一次设很多订阅。也就是说，这些订阅的更新都可以被订阅者收到。订阅者也可以取消订阅。订阅一般来说是可打印字符串。其他细节可以参考{{zmq_setsockopt[3]}}。

发布-订阅套接字是异步的。client在循环中（也可以只做一次）做{{zmq_recv[3]}}。尝试用SUB套接字发送消息会导致错误。同样，service按需做{{zmq_send[3]}}，但是绝对不可以用PUB套接字做{{zmq_recv[3]}}。

理论上来说，0MQ的套接字是无所谓哪边绑定哪边连接的。但是，实际上有一些没有文档说明的细微差别，后续我将介绍。目前来说，除非你的网络设计不允许，否则都是PUB绑定，SUB连接。

关于PUB-SUB，还有一个很重要的事：你并不知道订阅者何时取消息。即使你先启动了订阅者，等待一会，再启动发布者，**订阅者总是会错失发布者发的第一条消息**。这是因为当订阅者连接到发布者的时间间隙（虽然很快，但还不是无延迟），发布者已经启动并开始发送消息了。

这个“慢连接”现象坑到了不少人，所以我们有必要更进一步解释清楚。记住，0MZ在后台做的是异步I/O。假如你有两个节点按顺序做以下事情：

* 订阅者连接到endpoint（端点），接收消息并计数。
* 发布者绑定到entpoint（端点），立刻开始发送1000条消息。

那么，订阅者可能什么都收不到。你困惑的检查是否设置了正确的过滤，然后再试一次，订阅者还是什么也收不到。

建立TCP连接需要三次握手，这可能会花费上几毫秒的时间，具体取决于你的网络环境以及经过的节点数。这些时间足够0MQ发送很多消息了。假设建立连接花费了5毫秒，而同样的连接，0MQ每秒可以发送1百万条消息。那么订阅者连接到发布者的5毫秒中，发布者只用了1毫秒就发完了1000条消息。

在[#套接字与模式]中我会解释如何使发布者和订阅者同步，只有当订阅者准备好时发布者才会开始发送消息。有一种简单的方法来同步PUB和SUB，就是让PUB延迟一段时间再发送消息。但是现实编程中我不建议使用这种方式，因为它太脆弱了，而且不好控制。不过这里我们先暂且使用sleep的方式来解决，等到[#套接字与模式]再讲述正确的处理方式。

另一种同步的方式则是认为发布者的消息流是无穷无尽的，因此丢失了前面一部分信息也没有关系。我们的气象信息客户端就是这么做的。

So the client subscribes to its chosen zip code and collects a thousand updates for that zip code. That means about ten million updates from the server, if zip codes are randomly distributed. You can start the client, and then the server, and the client will keep working. You can stop and restart the server as often as you like, and the client will keep working. When the client has collected its thousand updates, it calculates the average, prints it, and exits.
因此client会选择订阅的邮编，接收来自这个邮编的1000条消息。也就是说，如果邮编是随机生成的，大概需要发送1千万条消息。你可以先启动client，再启动server，那么client仍可以正常工作。如果你乐意的话，停下再重启server多少次也没关系，client照样正常工作。当client收集满1000条消息，它将计算平均值、打印并退出程序。

Some points about the publish-subscribe (pub-sub) pattern:
关于发布-订阅（PUB-SUB）模式的几点说明：

* A subscriber can connect to more than one publisher, using one connect call each time. Data will then arrive and be interleaved ("fair-queued") so that no single publisher drowns out the others.

* If a publisher has no connected subscribers, then it will simply drop all messages.

* If you're using TCP and a subscriber is slow, messages will queue up on the publisher. We'll look at how to protect publishers against this using the "high-water mark" later.

* From 0MQ v3.x, filtering happens at the publisher side when using a connected protocol ({{tcp://}} or {{ipc://}}). Using the {{epgm://}} protocol, filtering happens at the subscriber side. In 0MQ v2.x, all filtering happened at the subscriber side.

This is how long it takes to receive and filter 10M messages on my laptop, which is an 2011-era Intel i5, decent but nothing special:

[[code]]
$ time wuclient
Collecting updates from weather server...
Average temperature for zipcode '10001 ' was 28F

real    0m4.470s
user    0m0.000s
sys     0m0.008s
[[/code]]

++ Divide and Conquer
++ 分而治之

[[code type="textdiagram" title="Parallel Pipeline"]]
            #-------------#
            |  Ventilator |
            +-------------+
            |    PUSH     |
            '------+------'
                   |
                 tasks
                   |
      .------------+-------------.
      |            |             |
      v            v             v
.----------.  .----------.  .----------.
|   PULL   |  |   PULL   |  |   PULL   |
+----------+  +----------+  +----------+
|  Worker  |  |  Worker  |  |  Worker  |
+----------+  +----------+  +----------+
|   PUSH   |  |   PUSH   |  |   PUSH   |
'----+-----'  '----+-----'  '----+-----'
      |            |             |
      '------------+-------------'
                   |
                results
                   |
                   v
            .-------------.
            |    PULL     |
            +-------------+
            |    Sink     |
            #-------------#
[[/code]]

As a final example (you are surely getting tired of juicy code and want to delve back into philological discussions about comparative abstractive norms), let's do a little supercomputing. Then coffee. Our supercomputing application is a fairly typical parallel processing model[figure]. We have:

* A ventilator that produces tasks that can be done in parallel
* A set of workers that process tasks
* A sink that collects results back from the worker processes

In reality, workers run on superfast boxes, perhaps using GPUs (graphic processing units) to do the hard math. Here is the ventilator. It generates 100 tasks, each one is a message telling the worker to sleep for some number of milliseconds:

[[code type="example" title="Parallel task ventilator" name="taskvent"]]
[[/code]]

Here is the worker application. It receives a message, sleeps for that number of seconds, and then signals that it's finished:

[[code type="example" title="Parallel task worker" name="taskwork"]]
[[/code]]

Here is the sink application. It collects the 100 tasks, then calculates how long the overall processing took, so we can confirm that the workers really were running in parallel if there are more than one of them:

[[code type="example" title="Parallel task sink" name="tasksink"]]
[[/code]]

The average cost of a batch is 5 seconds. When we start 1, 2, or 4 workers we get results like this from the sink:

* 1 worker: total elapsed time: 5034 msecs.
* 2 workers: total elapsed time: 2421 msecs.
* 4 workers: total elapsed time: 1018 msecs.

Let's look at some aspects of this code in more detail:

* The workers connect upstream to the ventilator, and downstream to the sink. This means you can add workers arbitrarily. If the workers bound to their endpoints, you would need (a) more endpoints and (b) to modify the ventilator and/or the sink each time you added a worker. We say that the ventilator and sink are //stable// parts of our architecture and the workers are //dynamic// parts of it.

* We have to synchronize the start of the batch with all workers being up and running. This is a fairly common gotcha in 0MQ and there is no easy solution. The {{zmq_connect}} method takes a certain time. So when a set of workers connect to the ventilator, the first one to successfully connect will get a whole load of messages in that short time while the others are also connecting. If you don't synchronize the start of the batch somehow, the system won't run in parallel at all. Try removing the wait in the ventilator, and see what happens.

* The ventilator's PUSH socket distributes tasks to workers (assuming they are all connected //before// the batch starts going out) evenly. This is called //load balancing// and it's something we'll look at again in more detail.

* The sink's PULL socket collects results from workers evenly. This is called //fair-queuing//[figure].

[[code type="textdiagram" title="Fair Queuing"]]
#---------#   #---------#   #---------#
|  PUSH   |   |  PUSH   |   |  PUSH   |
'----+----'   '----+----'   '----+----'
     |             |             |
 R1, R2, R3       R4           R5, R6
     |             |             |
     '-------------+-------------'
                   |
             fair-queuing
         R1, R4, R5, R2, R6, R3
                   |
                   v
            .-------------.
            |     PULL    |
            #-------------#
[[/code]]

The pipeline pattern also exhibits the "slow joiner" syndrome, leading to accusations that PUSH sockets don't load balance properly. If you are using PUSH and PULL, and one of your workers gets way more messages than the others, it's because that PULL socket has joined faster than the others, and grabs a lot of messages before the others manage to connect. If you want proper load balancing, you probably want to look at the The load balancing pattern in [#advanced-request-reply].

++ Programming with 0MQ
++ 用0MQ编程

Having seen some examples, you must be eager to start using 0MQ in some apps. Before you start that, take a deep breath, chillax, and reflect on some basic advice that will save you much stress and confusion.

* Learn 0MQ step-by-step. It's just one simple API, but it hides a world of possibilities. Take the possibilities slowly and master each one.

* Write nice code. Ugly code hides problems and makes it hard for others to help you. You might get used to meaningless variable names, but people reading your code won't. Use names that are real words, that say something other than "I'm too careless to tell you what this variable is really for". Use consistent indentation and clean layout. Write nice code and your world will be more comfortable.

* Test what you make as you make it. When your program doesn't work, you should know what five lines are to blame. This is especially true when you do 0MQ magic, which just //won't// work the first few times you try it.

* When you find that things don't work as expected, break your code into pieces, test each one, see which one is not working. 0MQ lets you make essentially modular code; use that to your advantage.

* Make abstractions (classes, methods, whatever) as you need them. If you copy/paste a lot of code, you're going to copy/paste errors, too.

+++ Getting the Context Right
+++ 正确的处理Context（上下文）

0MQ applications always start by creating a //context//, and then using that for creating sockets. In C, it's the {{zmq_ctx_new[3]}} call. You should create and use exactly one context in your process. Technically, the context is the container for all sockets in a single process, and acts as the transport for {{inproc}} sockets, which are the fastest way to connect threads in one process. If at runtime a process has two contexts, these are like separate 0MQ instances. If that's explicitly what you want, OK, but otherwise remember:

**Do one {{zmq_ctx_new[3]}} at the start of your main line code, and one {{zmq_ctx_destroy[3]}} at the end.**

If you're using the {{fork()}} system call, each process needs its own context. If you do {{zmq_ctx_new[3]}} in the main process before calling {{fork()}}, the child processes get their own contexts. In general, you want to do the interesting stuff in the child processes and just manage these from the parent process.

+++ Making a Clean Exit
+++ 干净的退出

Classy programmers share the same motto as classy hit men: always clean-up when you finish the job. When you use 0MQ in a language like Python, stuff gets automatically freed for you. But when using C, you have to carefully free objects when you're finished with them or else you get memory leaks, unstable applications, and generally bad karma.

Memory leaks are one thing, but 0MQ is quite finicky about how you exit an application. The reasons are technical and painful, but the upshot is that if you leave any sockets open, the {{zmq_ctx_destroy[3]}} function will hang forever. And even if you close all sockets, {{zmq_ctx_destroy[3]}} will by default wait forever if there are pending connects or sends unless you set the LINGER to zero on those sockets before closing them.

The 0MQ objects we need to worry about are messages, sockets, and contexts. Luckily it's quite simple, at least in simple programs:

* Use {{zmq_send[3]}} and {{zmq_recv[3]}} when you can, it avoids the need to work with zmq_msg_t objects.

* If you do use {{zmq_msg_recv[3]}}, always release the received message as soon as you're done with it, by calling {{zmq_msg_close[3]}}.

* If you are opening and closing a lot of sockets, that's probably a sign that you need to redesign your application. In some cases socket handles won't be freed until you destroy the context.

* When you exit the program, close your sockets and then call {{zmq_ctx_destroy[3]}}. This destroys the context.

This is at least the case for C development. In a language with automatic object destruction, sockets and contexts will be destroyed as you leave the scope. If you use exceptions you'll have to do the clean-up in something like a "final" block, the same as for any resource.

If you're doing multithreaded work, it gets rather more complex than this. We'll get to multithreading in the next chapter, but because some of you will, despite warnings, try to run before you can safely walk, below is the quick and dirty guide to making a clean exit in a //multithreaded// 0MQ application.

First, do not try to use the same socket from multiple threads. Please don't explain why you think this would be excellent fun, just please don't do it. Next, you need to shut down each socket that has ongoing requests. The proper way is to set a low LINGER value (1 second), and then close the socket. If your language binding doesn't do this for you automatically when you destroy a context, I'd suggest sending a patch.

Finally, destroy the context. This will cause any blocking receives or polls or sends in attached threads (i.e., which share the same context) to return with an error. Catch that error, and then set linger on, and close sockets in //that// thread, and exit. Do not destroy the same context twice. The {{zmq_ctx_destroy}} in the main thread will block until all sockets it knows about are safely closed.

Voila! It's complex and painful enough that any language binding author worth his or her salt will do this automatically and make the socket closing dance unnecessary.

++ Why We Needed 0MQ
++ 为什么我们需要0MQ

Now that you've seen 0MQ in action, let's go back to the "why".

Many applications these days consist of components that stretch across some kind of network, either a LAN or the Internet. So many application developers end up doing some kind of messaging. Some developers use message queuing products, but most of the time they do it themselves, using TCP or UDP. These protocols are not hard to use, but there is a great difference between sending a few bytes from A to B, and doing messaging in any kind of reliable way.

Let's look at the typical problems we face when we start to connect pieces using raw TCP. Any reusable messaging layer would need to solve all or most of these:

* How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision. Blocking I/O creates architectures that do not scale well. But background I/O can be very hard to do right.

* How do we handle dynamic components, i.e., pieces that go away temporarily? Do we formally split components into "clients" and "servers" and mandate that servers cannot disappear? What then if we want to connect servers to servers? Do we try to reconnect every few seconds?

* How do we represent a message on the wire? How do we frame data so it's easy to write and read, safe from buffer overflows, efficient for small messages, yet adequate for the very largest videos of dancing cats wearing party hats?

* How do we handle messages that we can't deliver immediately? Particularly, if we're waiting for a component to come back online? Do we discard messages, put them into a database, or into a memory queue?

* Where do we store message queues? What happens if the component reading from a queue is very slow and causes our queues to build up? What's our strategy then?

* How do we handle lost messages? Do we wait for fresh data, request a resend, or do we build some kind of reliability layer that ensures messages cannot be lost? What if that layer itself crashes?

* What if we need to use a different network transport. Say, multicast instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or is the transport abstracted in some layer?

* How do we route messages? Can we send the same message to multiple peers? Can we send replies back to an original requester?

* How do we write an API for another language? Do we re-implement a wire-level protocol or do we repackage a library? If the former, how can we guarantee efficient and stable stacks? If the latter, how can we guarantee interoperability?

* How do we represent data so that it can be read between different architectures? Do we enforce a particular encoding for data types? How far is this the job of the messaging system rather than a higher layer?

* How do we handle network errors? Do we wait and retry, ignore them silently, or abort?

Take a typical open source project like [http://hadoop.apache.org/zookeeper/ Hadoop Zookeeper] and read the C API code in {{[http://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c src/c/src/zookeeper.c]}}. When I read this code, in January 2013, it was 4,200 lines of mystery and in there is an undocumented, client/server network communication protocol. I see it's efficient because it uses {{poll}} instead of {{select}}. But really, Zookeeper should be using a generic messaging layer and an explicitly documented wire level protocol. It is incredibly wasteful for teams to be building this particular wheel over and over.

But how to make a reusable messaging layer? Why, when so many projects need this technology, are people still doing it the hard way by driving TCP sockets in their code, and solving the problems in that long list over and over[figure]?

It turns out that building reusable messaging systems is really difficult, which is why few FOSS projects ever tried, and why commercial messaging products are complex, expensive, inflexible, and brittle. In 2006, iMatix designed [http://www.amqp.org AMQP] which started to give FOSS developers perhaps the first reusable recipe for a messaging system. AMQP works better than many other designs, [http://www.imatix.com/articles:whats-wrong-with-amqp but remains relatively complex, expensive, and brittle]. It takes weeks to learn to use, and months to create stable architectures that don't crash when things get hairy.

[[code type="textdiagram" title="Messaging as it Starts"]]
.------------.
|            |
|  Piece A   |
|            |
'------------'
      ^
      |
     TCP
      |
      v
.------------.
|            |
|  Piece B   |
|            |
'------------'
[[/code]]

Most messaging projects, like AMQP, that try to solve this long list of problems in a reusable way do so by inventing a new concept, the "broker", that does addressing, routing, and queuing. This results in a client/server protocol or a set of APIs on top of some undocumented protocol that allows applications to speak to this broker. Brokers are an excellent thing in reducing the complexity of large networks. But adding broker-based messaging to a product like Zookeeper would make it worse, not better. It would mean adding an additional big box, and a new single point of failure. A broker rapidly becomes a bottleneck and a new risk to manage. If the software supports it, we can add a second, third, and fourth broker and make some failover scheme. People do this. It creates more moving pieces, more complexity, and more things to break.

And a broker-centric setup needs its own operations team. You literally need to watch the brokers day and night, and beat them with a stick when they start misbehaving. You need boxes, and you need backup boxes, and you need people to manage those boxes. It is only worth doing for large applications with many moving pieces, built by several teams of people over several years.

[[code type="textdiagram" title="Messaging as it Becomes"]]
        .---.             .---.
.---.   |   |   .---.  ^  |   |
|   +-->|   |<--|   |  |  |   |
|   |   '---'   |   |  |  '-+-'
'-+-'           '-+-'  |    |
  |               ^    |    |
  |       .-------+----+----'
  |       |       |    |
  '-------+-------+----+--.
          |       |    |  |
  .-------+-------+----+--+-----.
  |       v       |       v     |
.-+-.   .---.     |     .---.   |
|   |   |   |   .-+-.   |   |-->|
|   +-->|   +-->|   +-->|   |   |
'---'   '---'   |   |   '---'   |
          ^     '-+-'     ^     |
          |       |       |     |
  .-------+-------+-------'     |
  |       |       |             |
  v     .-+-.     v     .---.   |
.---.   |   |   .---.   |   |   |
|   |<--|   |<--|   |<--|   |<--'
|   |   '---'   |   |   '---'
'---'           '---'
[[/code]]

So small to medium application developers are trapped. Either they avoid network programming and make monolithic applications that do not scale. Or they jump into network programming and make brittle, complex applications that are hard to maintain. Or they bet on a messaging product, and end up with scalable applications that depend on expensive, easily broken technology. There has been no really good choice, which is maybe why messaging is largely stuck in the last century and stirs strong emotions: negative ones for users, gleeful joy for those selling support and licenses[figure].

What we need is something that does the job of messaging, but does it in such a simple and cheap way that it can work in any application, with close to zero cost. It should be a library which which you just link, without any other dependencies. No additional moving pieces, so no additional risk. It should run on any OS and work with any programming language.

And this is 0MQ: an efficient, embeddable library that solves most of the problems an application needs to become nicely elastic across a network, without much cost.

Specifically:

* It handles I/O asynchronously, in background threads. These communicate with application threads using lock-free data structures, so concurrent 0MQ applications need no locks, semaphores, or other wait states.

* Components can come and go dynamically and 0MQ will automatically reconnect. This means you can start components in any order. You can create "service-oriented architectures" (SOAs) where services can join and leave the network at any time.

* It queues messages automatically when needed. It does this intelligently, pushing messages as close as possible to the receiver before queuing them.

* It has ways of dealing with over-full queues (called "high water mark"). When a queue is full, 0MQ automatically blocks senders, or throws away messages, depending on the kind of messaging you are doing (the so-called "pattern").

* It lets your applications talk to each other over arbitrary transports: TCP, multicast, in-process, inter-process. You don't need to change your code to use a different transport.

* It handles slow/blocked readers safely, using different strategies that depend on the messaging pattern.

* It lets you route messages using a variety of patterns such as request-reply and pub-sub. These patterns are how you create the topology, the structure of your network.

* It lets you create proxies to queue, forward, or capture messages with a single call. Proxies can reduce the interconnection complexity of a network.

* It delivers whole messages exactly as they were sent, using a simple framing on the wire. If you write a 10k message, you will receive a 10k message.

* It does not impose any format on messages. They are blobs of zero to gigabytes large. When you want to represent data you choose some other product on top, such as msgpack, Google's protocol buffers, and others.

* It handles network errors intelligently, by retrying automatically in cases where it makes sense.

* It reduces your carbon footprint. Doing more with less CPU means your boxes use less power, and you can keep your old boxes in use for longer. Al Gore would love 0MQ.

Actually 0MQ does rather more than this. It has a subversive effect on how you develop network-capable applications. Superficially, it's a socket-inspired API on which you do {{zmq_recv[3]}} and {{zmq_send[3]}}. But message processing rapidly becomes the central loop, and your application soon breaks down into a set of message processing tasks. It is elegant and natural. And it scales: each of these tasks maps to a node, and the nodes talk to each other across arbitrary transports. Two nodes in one process (node is a thread), two nodes on one box (node is a process), or two boxes on one network (node is a box)--it's all the same, with no application code changes.

++ Socket Scalability
++ 套接字的扩展性

Let's see 0MQ's scalability in action. Here is a shell script that starts the weather server and then a bunch of clients in parallel:

[[code]]
wuserver &
wuclient 12345 &
wuclient 23456 &
wuclient 34567 &
wuclient 45678 &
wuclient 56789 &
[[/code]]

As the clients run, we take a look at the active processes using the {{top}} command', and we see something like (on a 4-core box):

[[code]]
PID  USER  PR  NI  VIRT  RES  SHR S %CPU %MEM   TIME+  COMMAND
7136  ph   20   0 1040m 959m 1156 R  157 12.0 16:25.47 wuserver
7966  ph   20   0 98608 1804 1372 S   33  0.0  0:03.94 wuclient
7963  ph   20   0 33116 1748 1372 S   14  0.0  0:00.76 wuclient
7965  ph   20   0 33116 1784 1372 S    6  0.0  0:00.47 wuclient
7964  ph   20   0 33116 1788 1372 S    5  0.0  0:00.25 wuclient
7967  ph   20   0 33072 1740 1372 S    5  0.0  0:00.35 wuclient
[[/code]]

Let's think for a second about what is happening here. The weather server has a single socket, and yet here we have it sending data to five clients in parallel. We could have thousands of concurrent clients. The server application doesn't see them, doesn't talk to them directly. So the 0MQ socket is acting like a little server, silently accepting client requests and shoving data out to them as fast as the network can handle it. And it's a multithreaded server, squeezing more juice out of your CPU.

++ Upgrading from 0MQ v2.2 to 0MQ v3.2
++ 从0MQ v2.2升级到0MQ v3.2

+++ Compatible Changes
+++ 兼容的变化

These changes don't impact existing application code directly:

* Pub-sub filtering is now done at the publisher side instead of subscriber side. This improves performance significantly in many pub-sub use cases. You can mix v3.2 and v2.1/v2.2 publishers and subscribers safely.

* 0MQ v3.2 has many new API methods ({{zmq_disconnect[3]}}, {{zmq_unbind[3]}}, {{zmq_monitor[3]}}, {{zmq_ctx_set[3]}}, etc.)

+++ Incompatible Changes
+++ 不兼容的变化

These are the main areas of impact on applications and language bindings:

* Changed send/recv methods: {{zmq_send[3]}} and {{zmq_recv[3]}} have a different, simpler interface, and the old functionality is now provided by {{zmq_msg_send[3]}} and {{zmq_msg_recv[3]}}. Symptom: compile errors. Solution: fix up your code.

* These two methods return positive values on success, and -1 on error. In v2.x they always returned zero on success. Symptom: apparent errors when things actually work fine. Solution: test strictly for return code = -1, not non-zero.

* {{zmq_poll[3]}} now waits for milliseconds, not microseconds. Symptom: application stops responding (in fact responds 1000 times slower). Solution: use the {{ZMQ_POLL_MSEC}} macro defined below, in all {{zmq_poll}} calls.

* {{ZMQ_NOBLOCK}} is now called {{ZMQ_DONTWAIT}}. Symptom: compile failures on the {{ZMQ_NOBLOCK}} macro.

* The {{ZMQ_HWM}} socket option is now broken into {{ZMQ_SNDHWM}} and {{ZMQ_RCVHWM}}.  Symptom: compile failures on the {{ZMQ_HWM}} macro.

* Most but not all {{zmq_getsockopt[3]}} options are now integer values. Symptom: runtime error returns on {{zmq_setsockopt}} and {{zmq_getsockopt}}.

* The {{ZMQ_SWAP}} option has been removed. Symptom: compile failures on {{ZMQ_SWAP}}. Solution: redesign any code that uses this functionality.

+++ Suggested Shim Macros
+++ 建议的宏

For applications that want to run on both v2.x and v3.2, such as language bindings, our advice is to emulate c3.2 as far as possible. Here are C macro definitions that help your C/C++ code to work across both versions (taken from [http://czmq.zeromq.org CZMQ]):

[[code type="fragment" name="upgrade-shim"]]
#ifndef ZMQ_DONTWAIT
#   define ZMQ_DONTWAIT     ZMQ_NOBLOCK
#endif
#if ZMQ_VERSION_MAJOR == 2
#   define zmq_msg_send(msg,sock,opt) zmq_send (sock, msg, opt)
#   define zmq_msg_recv(msg,sock,opt) zmq_recv (sock, msg, opt)
#   define zmq_ctx_destroy(context) zmq_term(context)
#   define ZMQ_POLL_MSEC    1000        //  zmq_poll is usec
#   define ZMQ_SNDHWM ZMQ_HWM
#   define ZMQ_RCVHWM ZMQ_HWM
#elif ZMQ_VERSION_MAJOR == 3
#   define ZMQ_POLL_MSEC    1           //  zmq_poll is msec
#endif
[[/code]]

++ Warning: Unstable Paradigms!
++ 警告：思维定势不稳

Traditional network programming is built on the general assumption that one socket talks to one connection, one peer. There are multicast protocols, but these are exotic. When we assume "one socket = one connection", we scale our architectures in certain ways. We create threads of logic where each thread work with one socket, one peer. We place intelligence and state in these threads.

In the 0MQ universe, sockets are doorways to fast little background communications engines that manage a whole set of connections automagically for you. You can't see, work with, open, close, or attach state to these connections. Whether you use blocking send or receive, or poll, all you can talk to is the socket, not the connections it manages for you. The connections are private and invisible, and this is the key to 0MQ's scalability.

This is because your code, talking to a socket, can then handle any number of connections across whatever network protocols are around, without change. A messaging pattern sitting in 0MQ scales more cheaply than a messaging pattern sitting in your application code.

So the general assumption no longer applies. As you read the code examples, your brain will try to map them to what you know. You will read "socket" and think "ah, that represents a connection to another node". That is wrong. You will read "thread" and your brain will again think, "ah, a thread represents a connection to another node", and again your brain will be wrong.

If you're reading this Guide for the first time, realize that until you actually write 0MQ code for a day or two (and maybe three or four days), you may feel confused, especially by how simple 0MQ makes things for you, and you may try to impose that general assumption on 0MQ, and it won't work. And then you will experience your moment of enlightenment and trust, that //zap-pow-kaboom// satori paradigm-shift moment when it all becomes clear.
